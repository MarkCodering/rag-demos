{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/Documents/rag-demos/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mark/Documents/rag-demos/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]\n",
      "/tmp/ipykernel_60257/2380910739.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 100 documents to the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]\n",
      "/home/mark/Documents/rag-demos/.venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, it appears that GPT-4 is a model in the GPT (Generative Pre-trained Transformer) family, specifically a successor to GPT-3.5.\n",
      "\n",
      "GPT-4 is a large-scale language model trained on a massive dataset of text, which enables it to generate human-like language outputs. The model is designed to understand and respond to natural language inputs, such as sentences, paragraphs, or even entire documents.\n",
      "\n",
      "The \"Turbo\" suffix in GPT3.5-Turbo suggests that it is a variant of GPT-3.5 with additional capabilities or improvements. The \"launch 98\" part is likely a version number or a specific iteration of the model.\n",
      "\n",
      "Without more information, it's difficult to provide a detailed explanation of how GPT-4 works. However, here is a general overview of the GPT architecture:\n",
      "\n",
      "1. **Pre-training**: GPT-4 is trained on a large corpus of text data, which allows it to learn patterns and relationships between words, phrases, and sentences.\n",
      "2. **Transformer architecture**: GPT-4 uses a transformer-based architecture, which is a type of recurrent neural network (RNN) that's particularly well-suited for natural language processing tasks.\n",
      "3. **Self-supervised learning**: GPT-4 is trained using a self-supervised learning approach, where the model is trained to predict the next word in a sequence, given the context of the previous words.\n",
      "4. **Fine-tuning**: GPT-4 can be fine-tuned on specific tasks or datasets, which allows it to adapt to specific domains or languages.\n",
      "\n",
      "The results shown in Figure 11 likely evaluate the performance of GPT-4 on various tasks, such as language translation, text classification, or generation.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install bitsandbytes\n",
    "# pip install -qU langchain_community pypdf\n",
    "# pip install -qU \"langchain-chroma>=0.1.2\"\n",
    "# pip install -U sentence-transformers\n",
    "# pip install sentence_transformers\n",
    "# pip install langchain==0.0.174 -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com\n",
    "\n",
    "from uuid import uuid4\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "prompt = \"What is GPT4 and how does it work?\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"Snowflake/snowflake-arctic-embed-m\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    \"./data/gpt4.pdf\",\n",
    ")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"gpt4\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "num_pages = len(document)\n",
    "for i in range(num_pages):\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=document[i].page_content,\n",
    "            id=i,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"Adding {len(documents)} documents to the vector store\")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    prompt,\n",
    "    k=1,\n",
    ")\n",
    "context = []\n",
    "for res in results:\n",
    "    context.append(res.page_content)\n",
    "\n",
    "print(\"Context:\")\n",
    "print(len(context))\n",
    "\n",
    "# Ensure prompt and context are properly formatted\n",
    "prompt = prompt.replace(\"\\n\", \" \").strip()\n",
    "context = [c.replace(\"\\n\", \" \").strip() for c in context]\n",
    "# Make context a string from the list\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is the context to the question, please answer the prompt based on the context: {context[0]}\"},\n",
    "]\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", load_in_4bit=True, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, do_sample=True, max_length=1024, pad_token_id=tokenizer.eos_token_id, top_k=50, top_p=0.95, temperature=0.5, num_return_sequences=1)\n",
    "result = pipe(messages)[0]\n",
    "for message in result['generated_text']:\n",
    "    if message['role'] == 'assistant':\n",
    "        print(message['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
