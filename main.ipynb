{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 01:01:35.262456: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-05 01:01:35.304098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/tmp/ipykernel_35625/2821227197.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\", model_kwargs = {'device': 'cuda'}, show_progress=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff3add645ec4d8fac161990bd444c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14402aa9297c4b25a07b2a6967bbfdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "© Space Exploration Technologies Corp.  All rights reserved.  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "COPYRIGHT  \n",
      "Subject to the existing rights of third parties, Space Exploration Technologies Corp. (SpaceX) is the owner of the copyright \n",
      "in this work, and no portion hereof is to be copied, reproduced, or disseminated without the prior written consent of \n",
      "SpaceX.  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f47d314231b43f5b98b21e2bb39b81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful engineer who is trying to help a user with a problem. The user has a problem statement What is Falcon 9. The user has provided the following context:  \n",
      "© Space Exploration Technologies Corp.  All rights reserved.  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "COPYRIGHT  \n",
      "Subject to the existing rights of third parties, Space Exploration Technologies Corp. (SpaceX) is the owner of the copyright \n",
      "in this work, and no portion hereof is to be copied, reproduced, or disseminated without the prior written consent of \n",
      "SpaceX.  \n",
      ". Please provide a helpful response to the user.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install bitsandbytes\n",
    "# pip install -qU langchain_community pypdf\n",
    "# pip install -qU \"langchain-chroma>=0.1.2\"\n",
    "# pip install -U sentence-transformers\n",
    "# pip install sentence_transformers\n",
    "# pip install langchain==0.0.174 -i https://pypi.doubanio.com/simple/ --trusted-host pypi.doubanio.com\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\", model_kwargs = {'device': 'cuda'}, show_progress=True)\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    \"./data/falcon-users-guide-2021-09-compressed.pdf\",\n",
    ")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=document[0].page_content,\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "]\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    prompt,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "context = results[0].page_content\n",
    "\n",
    "print(results[0].page_content)\n",
    "    \n",
    "rag_prompt = f\"You are a helpful engineer who is trying to help a user with a problem. The user has a problem statement {prompt}. The user has provided the following context: {context}. Please provide a helpful response to the user.\"\n",
    "\n",
    "pipe = pipeline(model=\"meta-llama/Meta-Llama-3-8B\", model_kwargs={\"load_in_4bit\": True}, device_map=\"auto\")\n",
    "output = pipe(rag_prompt, do_sample=True, top_p=0.95)\n",
    "\n",
    "print(\"\\n\" + output[0][\"generated_text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def echo(input_text):\n",
    "    prompt = f\"You are a helpful engineer who is trying to help a user with a problem. The user has a problem statement {input_text}. The user has provided the following context: {context}. Please provide a helpful response to the user.\"\n",
    "\n",
    "    return pipe(prompt, do_sample=True, top_p=0.95)[0][\"generated_text\"]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=echo,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
